{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Working with raster data in python\n\n\n## Table of Contents\n\n1. [About the dataset](#dataset)<br>\n2. [Part 1 - Weather maps with netCDF4 and matplotlib](#part1)<br>\n    2.1. [Import packages](#import1)<br>\n    2.2. [Load gridded data with netCDF4](#load1)<br>\n    2.3. [Create a global map of the average temperature in January using matplotlib](#map1)<br>\n\n3. [Part 2 - Weather maps with xarray and Cartopy](#part2)<br>\n    3.1. [Import packages](#import2)<br>\n    3.2. [Load gridded data with xarray](#load2)<br>\n    3.3. [Create maps using xarray](#map21)<br>\n    3.4. [Create maps using Cartoid](#map22)<br>\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"dataset\"></a>\n## About the dataset\n\nWith the gridded data from [CRU](http://www.cru.uea.ac.uk/data/) you will learn how to work with gridded historical data. \n\nThe [dataset](https://crudata.uea.ac.uk/cru/data/temperature/#datdow) contains a 5&deg; by 5&deg; grid with absolute temperatures from 1961 to 1990. The data is represented in a [NetCDF](https://pro.arcgis.com/en/pro-app/help/data/multidimensional/what-is-netcdf-data.htm) format.\n\nDownload the following file, and store it locally or in object-store when working on the [IBM Data Science Experience](https://datascience.ibm.com/) :\n\n* https://crudata.uea.ac.uk/cru/data/temperature/absolute.nc\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"part1\"></a>\n## Part 1 -  Weather maps with netCDF4 and matplotlib\n\n\nIn the first half of this tutorial, we will see how to use Python's [netCDF4](https://unidata.github.io/netcdf4-python/netCDF4/index.html) module to extract data from the dataset. \n\n\n\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"import1\"></a>\n### 1. Import packages\n\nFollowing is the explicit list of imports that we used through this notebook.  "
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "import pandas as pd\nimport requests, json\nfrom io import StringIO\nfrom netCDF4 import Dataset\nimport numpy as np\nimport scipy\nimport matplotlib\nfrom pylab import *\nfrom mpl_toolkits.basemap import Basemap, addcyclic, shiftgrid\n%matplotlib inline",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We first import the dataset with a helper function that uses the project token created per instructions mentioned above. Import the `absolute.nc` file locally or add the below code by clicking on `Insert to code` below the file under the file in object-store. Then load the data and explore the variables and dimensions of the file. \n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\n# define the helper function \ndef download_file_to_local(project_filename, local_file_destination=None, project=None):\n    \"\"\"\n    Uses project-lib to get a bytearray and then downloads this file to local.\n    Requires a valid `project` object.\n    \n    Args:\n        project_filename str: the filename to be passed to get_file\n        local_file_destination: the filename for the local file if different\n        \n    Returns:\n        0 if everything worked\n    \"\"\"\n    \n    project = project\n    \n    # get the file\n    print(\"Attempting to get file {}\".format(project_filename))\n    _bytes = project.get_file(project_filename).read()\n    \n    # check for new file name, download the file\n    print(\"Downloading...\")\n    if local_file_destination==None: local_file_destination = project_filename\n    \n    with open(local_file_destination, 'wb') as f: \n        f.write(bytearray(_bytes))\n        print(\"Completed writing to {}\".format(local_file_destination))\n        \n    return 0",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "download_file_to_local('absolute.nc', project=project)\ncfile = \"absolute.nc\"",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"load1\"></a>\n### 2. Load gridded data with netCDF4\n\n We then use netCDF4's *Dictionary* collection to analyse the data and its relations between the fields that consitute the netCDF file. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = Dataset(cfile)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\nTo extract the data model version of the netCDF file, we use the *data_model* variable is used. The data model can be one of NETCDF3_CLASSIC, NETCDF4, NETCDF4_CLASSIC, NETCDF3_64BIT_OFFSET OR NETCDF3_63BIT_DATA.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(dataset.data_model)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "*dimensions* returns a dictionary with variables names from the dataset mapped to instances of the Dimensions class. It provides the name of the variable along with its size."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(dataset.dimensions)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "*variables* returns a dictionary that maps the variable names from the dataset as instances of *Variable* class."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(dataset.variables)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Below is an example of how variables from the dataset can be accessed as keys of the dictionary returned in the line above. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "lons = dataset.variables['lon'][:] \nprint(\"Shape of longitude data : \",np.shape(lons))\n\nlats = dataset.variables['lat'][:] \nprint(\"Shape of latitude data : \",np.shape(lats))\n\ntime = dataset.variables['time'][:] \nprint(\"Shape of time data : \",np.shape(time))\n\ntemperature = dataset.variables['tem'][:,:,:]\nprint(\"Shape of temperature data : \",np.shape(temperature))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"map1\"></a>\n### 3. Create a global map of the average temperature in January using matplotlib\n\n\nWe will now see how matplotlib and its extensions can be used to plot 2D maps in Python. Here we use the matplotlib [basemap](https://matplotlib.org/basemap/users/intro.html) toolkit. To map the points on a 2D surface, basemap supports 24 different types of [projections](https://matplotlib.org/basemap/users/mapsetup.html). In this example Miller Projections is used. Miller projections are generally used for wall maps rather than as navigational maps. Details of Miller projections can be found [here](https://matplotlib.org/basemap/users/mill.html). llcrnrlon, llcrnrlat refer to longitude and latitude of lower left hand corner of the desired map domain(degrees) respectively.  urcrnrlon, urcrnrlat refer to longitude and latitude of lower right hand corner of the desired map domain(degrees) respectively. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# define the area to plot and projection to use\nm =\\\nBasemap(llcrnrlon=-180,llcrnrlat=-60,urcrnrlon=180,urcrnrlat=80,projection='mill')\n\n# covert the latitude, longitude and temperatures to raster coordinates to be plotted\nt1=temperature[0,:,:]\nt1,lon=addcyclic(t1,lons)\njanuary,longitude=shiftgrid(180.,t1,lon,start=False)\nx,y=np.meshgrid(longitude,lats)\npx,py=m(x,y)\n\npalette=cm.RdYlBu_r\nrmin=-30.\nrmax=30.\nncont=20\ndc=(rmax-rmin)/ncont\nvc=arange(rmin,rmax+dc,dc)\npal_norm=matplotlib.colors.Normalize(vmin = rmin, vmax = rmax, clip = False)\n\nm.drawcoastlines(linewidth=0.5)\nm.drawmapboundary(fill_color=(1.0,1.0,1.0))\ncf=m.pcolormesh(px, py, january, cmap = palette)\ncbar=colorbar(cf,orientation='horizontal', shrink=0.95)\ncbar.set_label('Mean Temperature in January')\ntight_layout()\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "*addcyclic* adds a column of longitude to a set of data. In the code below we see that the longitude array is added to an array containing temperature entries. *shiftgrid* moves all longitudes and data east or west. The *meshgrid* method returns co-ordinate matrictes from one dimentional coordinate arrays. In the code below, we use meshgrid to convert longitude and latitude arrays into x and y coordinate arrays. "
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"part2\"></a>\n## Part 2 - Weather maps with xarray and Cartopy\n\nIn the second half of tutorial, we will see how to use [xarray](http://xarray.pydata.org/en/stable/) to process the netCDF data. xarray is useful with analysing multidimensional arrays. It shares functionalities from pandas and NumPy. xarray has proven to be a robust library to handle netCDF files. \n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"import2\"></a>\n### 1. Import packages\n\nFollowing snippet shows the required imports that needs to be done to be able to run the notebook. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import numpy as np\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\n%matplotlib inline",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"load2\"></a>\n### 2. Load gridded data with xarray\n\nWe then open and load the dataset using xarray. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset = xr.open_dataset(cfile)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "xarray supports the following data structures : \n\n- *DataArray* which is a multidimensional array \n-  *Dataset* which is a dictionaty of multiple DataArray objects.\n\n netCDF data is represented as a Dataset in xarray. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset.values",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "*dims* returns the value of the x, y and z coordinates. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset.dims",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "*coords* returns just the coordinates section from the *values* variable we saw above."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset.coords",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\nSince xarray is an extension to pandas, it offers a method which enables us to convert the dataset to a dataframe. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = dataset.to_dataframe()\ndf.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.describe()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"map21\"></a>\n### 3. Create maps using xarray\nxarray also supports plotting fuctionalities by extending the *matplotlib* library. DataArray objects can be plotted using xarray libraries. To plot Dataset objects, the relevant DataArrays or dimensions needs to be accessed. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset.mean(dim=['time','lon']).to_dataframe().plot()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "dataset.tem[0].plot()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"map22\"></a>\n### 4. Create maps using Cartopy \n\n[Cartopy](https://scitools.org.uk/cartopy/docs/latest/) is one of the several  plotting applications that are compatible with xarray. Few others are Seaborn, HoloViews and GeoViews.\n\nBelow is a simple example of using cartopy to create visualizations. We compare the Molleweide projection vs the Miller projection. A complete list of projections can be found [here](https://scitools.org.uk/cartopy/docs/latest/crs/projections.html)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "f = plt.figure(figsize=(14,5))\nplt.title(\"Molleweide VS Miller Projection for the month of January\", fontsize=20)\nplt.axis('off')\nax1 = f.add_subplot(1,2,1, projection = ccrs.Mollweide())\nax2 = f.add_subplot(1,2,2, projection = ccrs.Miller())\nax1.coastlines()\nax1.gridlines()\nax2.coastlines()\nax2.gridlines()\n\ndataset.tem[0].plot(ax=ax1, transform=ccrs.PlateCarree())\ndataset.tem[0].plot(ax=ax2, transform=ccrs.PlateCarree())",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Following is a heat map comparing the intensity of temperatures between the month of January and June."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "proj = ccrs.Miller()\njan_june = dataset.tem.isel(time=[0,5])\nmonths = ['January','June']\ni = 0\n\np = jan_june.plot(transform=ccrs.PlateCarree(),\n             col='time', col_wrap=2, \n             aspect=dataset.dims['lon'] / dataset.dims['lat'],\n             subplot_kws={'projection': proj})\n\n\nfor ax in p.axes.flat:\n    ax.coastlines()\n    ax.gridlines()\n    ax.set_title(months[i])\n    i = i+1\n    \n    ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Author\nMargriet Groenendijk is a Data & AI Developer Advocate for IBM. She develops and presents talks and workshops about data science and AI. She is active in the local developer communities through attending, presenting and organising meetups. She has a background in climate science where she\u00a0explored large observational\u00a0datasets of carbon uptake by forests\u00a0during her PhD, and\u00a0global scale weather and climate models as a postdoctoral fellow.\u00a0\n\nSamaya Madhavan is an Advisory Software Engineer with IBM where she currently publishes content that are related to machine learning and deep learning. She is also a full stack software developer, experienced in offering AI based solutions within the healthcare domain. Samaya has her Bachelor of Engineering in Computer Science from College of Engineering, Guindy and her Master of Science in Computer Science from University of Texas at Arlington. She is an ardent learner and a very passionate algorithm solver.\n\nCopyright \u00a9 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}